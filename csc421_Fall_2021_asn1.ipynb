{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC421 Assignment 1 Agents, Search, and CSP \n",
    "\n",
    "This assignment notebook explores topics covered in **Chapter 2 - Intelligent Agents**, **Chapter 3 - Searching** and **Chapter 6 - Constraint Satisfication Problems** from the book *Artificial Intelligence: A Modern Approach.* The code provided is based on parts of the aima-code repository but has been modified and simplified for the purposes of the assignment. The notebook is self-contained and other than importing a few common packages you \n",
    "don't need to access any additional code. \n",
    "\n",
    "You are welcome and actually it can be educational to look at the code at the aima-code repository as well as other code resources you can find on the web. However, make sure you understand any code that you incoporate. \n",
    "\n",
    "The assignment structure is as follows - each item is worth 1 point: \n",
    "\n",
    "1. Agents (Basic) - Modify the BlindDog agent to sleep after eating (reflex agent->model-based agent) \n",
    "2. Agents (Expected) - Change the Environment from a simple 1D environment to a graph-based map environment \n",
    "3. Search (Basic) - Input Australia map, run BFS and DFS \n",
    "4. Search (Basic) - Print Frontier after each expansion \n",
    "5. Search (Expected) RandomSwitch search \n",
    "6. Search (Expected) Random generation + measurement \n",
    "7. Search (Advanced) Plotting \n",
    "8. CSP (Basic): Make example of job scheduling (or some other problem) and run CSP to find solution \n",
    "9. CSP (Expected): Translate complex contraint (maybe alldiff or binary <) to set of binary contraints \n",
    "10. CSP (Advanced): Implement CSP as a search problem and compare DFS with the CSP solver provided  \n",
    "\n",
    "The grading will be done in 0.5 increments. 1 point for correct answer, 0.5 points for partial or incorrect \n",
    "but reasonable answer and 0.0 for no answer or completely wrong answer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining abstract classes for Things, Agents, and Environments. Then we create a specific Park environment and a specific Agent (Blind Dog) that operates in this environment. \n",
    "Once that is done we can simulate how the agent interacts with the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Thing:                                                                                        \n",
    "    \"\"\"This represents any physical object that can appear in an Environment.                       \n",
    "    You subclass Thing to get the things you want. Each thing can have a                            \n",
    "    .__name__  slot (used for output only).\"\"\"                                                                                                                                                      \n",
    "    def __repr__(self):                                                                             \n",
    "        return '<{}>'.format(getattr(self, '__name__', self.__class__.__name__))                    \n",
    "                                                                                                                                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Thing):\n",
    "    \"\"\"An Agent is a subclass of Thing with one required instance attribute \n",
    "    (aka slot), .program, which should hold a function that takes one argument,\n",
    "    the percept, and returns an action. (What counts as a percept or action \n",
    "    will depend on the specific environment in which the agent exists.)\n",
    "    Note that 'program' is a slot, not a method. If it were a method, then the\n",
    "    program could 'cheat' and look at aspects of the agent. It's not supposed\n",
    "    to do that: the program can only look at the percepts. An agent program\n",
    "    that needs a model of the world (and of the agent itself) will have to\n",
    "    build and maintain its own model. There is an optional slot, .performance,\n",
    "    which is a number giving the performance measure of the agent in its\n",
    "    environment.\"\"\"\n",
    "\n",
    "    def __init__(self, program=None):\n",
    "        self.alive = True\n",
    "        self.holding = []\n",
    "        self.performance = 0\n",
    "        self.program = program\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \"\"\"Abstract class representing an Environment. 'Real' Environment classes\n",
    "    inherit from this. Your Environment will typically need to implement:\n",
    "        percept:           Define the percept that an agent sees.\n",
    "        execute_action:    Define the effects of executing an action.\n",
    "                           Also update the agent.performance slot.\n",
    "    The environment keeps a list of .things and .agents (which is a subset\n",
    "    of .things). Each agent has a .performance slot, initialized to 0.\n",
    "    Each thing has a .location slot, even though some environments may not\n",
    "    need this.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.things = []\n",
    "        self.agents = []\n",
    "\n",
    "    def thing_classes(self):\n",
    "        return []  # List of classes that can go into environment\n",
    "\n",
    "    def percept(self, agent):\n",
    "        \"\"\"Return the percept that the agent sees at this point. (Implement this.)\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def execute_action(self, agent, action):\n",
    "        \"\"\"Change the world to reflect this action. (Implement this.)\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def default_location(self, thing):\n",
    "        \"\"\"Default location to place a new thing with unspecified location.\"\"\"\n",
    "        return None\n",
    "\n",
    "    def is_done(self):\n",
    "        \"\"\"By default, we're done when we can't find a live agent.\"\"\"\n",
    "        return not any(agent.is_alive() for agent in self.agents)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Run the environment for one time step. \"\"\"\n",
    "        if not self.is_done():\n",
    "            actions = []\n",
    "            for agent in self.agents:\n",
    "                if agent.alive:\n",
    "                    actions.append(agent.program(self.percept(agent)))\n",
    "                else:\n",
    "                    actions.append(\"\")\n",
    "            for (agent, action) in zip(self.agents, actions):\n",
    "                self.execute_action(agent, action)\n",
    "\n",
    "    def run(self, steps=1000):\n",
    "        \"\"\"Run the Environment for given number of time steps.\"\"\"\n",
    "        for step in range(steps):\n",
    "            if self.is_done():\n",
    "                return\n",
    "            self.step()\n",
    "\n",
    "    def list_things_at(self, location, tclass=Thing):\n",
    "        \"\"\"Return all things exactly at a given location.\"\"\"\n",
    "        return [thing for thing in self.things\n",
    "                if thing.location == location and isinstance(thing, tclass)]\n",
    "\n",
    "    def add_thing(self, thing, location=None):\n",
    "        \"\"\"Add a thing to the environment, setting its location. For\n",
    "        convenience, if thing is an agent program we make a new agent\n",
    "        for it. (Shouldn't need to override this.)\"\"\"\n",
    "        if not isinstance(thing, Thing):\n",
    "            thing = Agent(thing)\n",
    "        if thing in self.things:\n",
    "            print(\"Can't add the same thing twice\")\n",
    "        else:\n",
    "            thing.location = location if location is not None else self.default_location(thing)\n",
    "            self.things.append(thing)\n",
    "            if isinstance(thing, Agent):\n",
    "                thing.performance = 0\n",
    "                self.agents.append(thing)\n",
    "\n",
    "    def delete_thing(self, thing):\n",
    "        \"\"\"Remove a thing from the environment.\"\"\"\n",
    "        try:\n",
    "            self.things.remove(thing)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            print(\"  in Environment delete_thing\")\n",
    "            print(\"  Thing to be removed: {} at {}\".format(thing, thing.location))\n",
    "            print(\"  from list: {}\".format([(thing, thing.location) for thing in self.things]))\n",
    "        if thing in self.agents:\n",
    "            self.agents.remove(thing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENVIRONMENT - Park\n",
    "\n",
    "A park is an example of an environment because our dog can perceive and act upon it. The <b>Environment</b> class is an abstract class, so we will have to create our own subclass from it before we can use it. In the cell below we create a specific Park environment that contains locations with food. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Food(Thing):\n",
    "    pass\n",
    "\n",
    "class Park(Environment):\n",
    "    def percept(self, agent):\n",
    "        '''return a list of things that are in our agent's location'''\n",
    "        things = self.list_things_at(agent.location)\n",
    "        return things\n",
    "    \n",
    "    def execute_action(self, agent, action):\n",
    "        '''changes the state of the environment based on what the agent does.'''\n",
    "        if action == \"move down\":\n",
    "            print('{} decided to {} at location: {}'.format(str(agent)[1:-1], action, agent.location))\n",
    "            agent.movedown()\n",
    "        elif action == \"eat\":\n",
    "            items = self.list_things_at(agent.location, tclass=Food)\n",
    "            if len(items) != 0:\n",
    "                if agent.eat(items[0]): #Have the dog eat the first item\n",
    "                    print('{} ate {} at location: {}'\n",
    "                          .format(str(agent)[1:-1], str(items[0])[1:-1], agent.location))\n",
    "                    self.delete_thing(items[0]) #Delete it from the Park after.\n",
    "\n",
    "    def is_done(self):\n",
    "        # simulation without explicit ending condition\n",
    "        return False\n",
    "    \n",
    "    def show(self, max_location): \n",
    "        for i in range(0,max_location): \n",
    "            print('{}:{}'.format(str(i), str(self.list_things_at(i))))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlindDog(Agent):\n",
    "    location = 1\n",
    "    \n",
    "    def movedown(self):\n",
    "        self.location += 1\n",
    "        \n",
    "    def eat(self, thing):\n",
    "        '''returns True upon success or False otherwise'''\n",
    "        if isinstance(thing, Food):\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a model-based agent - notice that we use a class that behaves \n",
    "# like a function i.e it is callable. That allows the use of a model that retains \n",
    "# information between calls to program. This is the program that the agent \n",
    "# executes to decide what action to choose based on the percepts received \n",
    "# notice that information between calls (the model of either the environment \n",
    "# or the agent maintains) can be stored in member variables. \n",
    "\n",
    "class HungryProgram:\n",
    "    \n",
    "    def __init__(self): \n",
    "        self.hungry = True \n",
    "\n",
    "    def program(self,percepts):\n",
    "        '''Returns an action based on it's percepts'''\n",
    "        for p in percepts: # first eat  - you're a dog!\n",
    "            if isinstance(p, Food):\n",
    "                self.hungry = False \n",
    "                return 'eat'\n",
    "        return 'move down'\n",
    "\n",
    "    def __call__(self, precepts): \n",
    "        return self.program(precepts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:[]\n",
      "1:[<BlindDog>]\n",
      "2:[]\n",
      "3:[<Food>]\n",
      "4:[]\n",
      "BlindDog decided to move down at location: 1\n",
      "0:[]\n",
      "1:[]\n",
      "2:[<BlindDog>]\n",
      "3:[<Food>]\n",
      "4:[]\n",
      "BlindDog decided to move down at location: 2\n",
      "0:[]\n",
      "1:[]\n",
      "2:[]\n",
      "3:[<BlindDog>, <Food>]\n",
      "4:[]\n",
      "BlindDog ate Food at location: 3\n",
      "0:[]\n",
      "1:[]\n",
      "2:[]\n",
      "3:[<BlindDog>]\n",
      "4:[]\n",
      "BlindDog decided to move down at location: 3\n",
      "0:[]\n",
      "1:[]\n",
      "2:[]\n",
      "3:[]\n",
      "4:[<BlindDog>]\n"
     ]
    }
   ],
   "source": [
    "# Now that we have some concrete implementation of an Agent and an Environment we can simulate the behavior \n",
    "# of the a specific BlindDog agent in that environment \n",
    "\n",
    "park = Park()\n",
    "dog = BlindDog(HungryProgram())\n",
    "food = Food()\n",
    "park.add_thing(dog, 1)\n",
    "park.add_thing(food, 3)\n",
    "park.show(5)\n",
    "park.run(1)\n",
    "park.show(5) \n",
    "park.run(1)\n",
    "park.show(5)\n",
    "park.run(1) \n",
    "park.show(5)\n",
    "park.run(1)\n",
    "park.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 1 (Basic) Agents 1.0 point \n",
    "### The BlindDog that knows when it is hungry  \n",
    "\n",
    "\n",
    "First make sure you understand how the code above works and especially the concrete implementation of the Park Environment and the Blind Dog. The current implementation of the Blind Dog checks each square to see if it perceives food. If it perceives food it eats it otherwise it moves down. Notice that currently the HungryDog program maintains a very simple model of the agent (whether it is hungry or not) but does not use that information in any way. Change the implementations provided above so that a new action \"move up\" action is supported by the Park Environment and Blind Dog. Then modify the HungryProgram so that when the dogs is not hungry it moves up instead of down. After it moves up after eating food then it becomes hungry again and returns to the original behavior of moving down when there is no food and eating when there is food. \n",
    "\n",
    "Your modified implementations should be named ParkNew, BlindDogNew, and HungryProgramNew. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:[]\n",
      "1:[<BlindNewDog>]\n",
      "2:[]\n",
      "3:[<Food>]\n",
      "4:[]\n",
      "BlindNewDog decided to move down at location: 1\n",
      "0:[]\n",
      "1:[]\n",
      "2:[<BlindNewDog>]\n",
      "3:[<Food>]\n",
      "4:[]\n",
      "BlindNewDog decided to move down at location: 2\n",
      "0:[]\n",
      "1:[]\n",
      "2:[]\n",
      "3:[<BlindNewDog>, <Food>]\n",
      "4:[]\n",
      "BlindNewDog ate Food at location: 3\n",
      "0:[]\n",
      "1:[]\n",
      "2:[]\n",
      "3:[<BlindNewDog>]\n",
      "4:[]\n",
      "BlindNewDog decided to move up at location: 3\n",
      "0:[]\n",
      "1:[]\n",
      "2:[<BlindNewDog>]\n",
      "3:[]\n",
      "4:[]\n"
     ]
    }
   ],
   "source": [
    "# Q1 ANSWSER GOES HERE (ParkNew, BlindNewGod, HungryProgramNew)\n",
    "\n",
    "class ParkNew(Environment):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.things = []\n",
    "        self.agents = []\n",
    "\n",
    "    def percept(self, agent):\n",
    "        '''return a list of things that are in our agent's location'''\n",
    "        things = self.list_things_at(agent.location)\n",
    "        return things\n",
    "    \n",
    "    def execute_action(self, agent, action):\n",
    "        '''changes the state of the environment based on what the agent does.'''\n",
    "\n",
    "        if action == \"move down\":\n",
    "            print('{} decided to {} at location: {}'.format(str(agent)[1:-1], action, agent.location))\n",
    "            agent.movedown()\n",
    "        elif action == \"move up\": # new action \"move up\" action is supported by the Park Environment \n",
    "            print('{} decided to {} at location: {}'.format(str(agent)[1:-1], action, agent.location))\n",
    "            agent.moveup()\n",
    "        elif action == \"eat\":\n",
    "            items = self.list_things_at(agent.location, tclass=Food)\n",
    "            if len(items) != 0:\n",
    "                if agent.eat(items[0]): #Have the dog eat the first item\n",
    "                    print('{} ate {} at location: {}'\n",
    "                          .format(str(agent)[1:-1], str(items[0])[1:-1], agent.location))\n",
    "                    self.delete_thing(items[0]) #Delete it from the Park after.\n",
    "\n",
    "    def is_done(self):\n",
    "        # simulation without explicit ending condition\n",
    "        return False\n",
    "    \n",
    "    def show(self, max_location): \n",
    "        for i in range(0,max_location): \n",
    "            print('{}:{}'.format(str(i), str(self.list_things_at(i))))\n",
    "\n",
    "\n",
    "class BlindNewDog(Agent):\n",
    "    location = 1\n",
    "    \n",
    "    def movedown(self):\n",
    "        self.location += 1\n",
    "    \n",
    "    def moveup(self):\n",
    "        self.location -=1  # new action \"move up\" action is supported by Blind Dog\n",
    "        \n",
    "    def eat(self, thing):\n",
    "        '''returns True upon success or False otherwise'''\n",
    "        if isinstance(thing, Food):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "class HungryProgramNew:\n",
    "    \n",
    "    def __init__(self): \n",
    "        self.hungry = True \n",
    "\n",
    "    def program(self,percepts):\n",
    "        '''Returns an action based on it's percepts'''\n",
    "        for p in percepts: # first eat  - you're a dog!\n",
    "            if isinstance(p, Food):\n",
    "                self.hungry = False \n",
    "                return 'eat'\n",
    "            if (self.hungry == False):\n",
    "                return 'move up'\n",
    "        return 'move down'\n",
    "\n",
    "\n",
    "    def __call__(self, precepts): \n",
    "        return self.program(precepts)\n",
    "\n",
    "park = ParkNew()\n",
    "dog = BlindNewDog(HungryProgramNew())\n",
    "food = Food()\n",
    "park.add_thing(dog, 1)\n",
    "park.add_thing(food, 3)\n",
    "park.show(5)\n",
    "park.run(1)\n",
    "park.show(5) \n",
    "park.run(1)\n",
    "park.show(5)\n",
    "park.run(1) \n",
    "park.show(5)\n",
    "park.run(1)\n",
    "park.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 2 Agents (Expected) 1.0 point \n",
    "\n",
    "The graph wandering blind dog. \n",
    "\n",
    "\n",
    "Use the code above as a template in order to create a new type of map environment. In this version, the blind dogs wanders around a map of locations represented as a graph. Replace the \"move down\" action with a \"move\" action. The blind dog moves from location to location. The \"move\" action selects randomly (with equal probability) one of the locations that are connected to the current city and moves the blind dog to it. Notice that the environment handles the random selection of a destination so this is a case of a non-deterministic environment but a deterministic agent. \n",
    "\n",
    "Each location of the map should represented by an integer number. Each location is connected to other \n",
    "neighboring locations. A dictionary is used to represent the connectivity information by representing \n",
    "the neighboring locations for a particular location as a list. \n",
    "\n",
    "For example consider the map of Australia we looked at in Chapter 6 Constraint Satisfication Problem. \n",
    "If we assign WA: 1, NT: 2, SA: 3, Q: 4, NSW: 5, V: 6 then we can represent the connectivity graph \n",
    "as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {2, 3}, 2: {1, 3, 4}, 3: {1, 2, 4, 5, 6}, 4: {2, 3, 4}, 5: {3, 4, 6}}\n",
      "2\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Python representation of the map of Australia connectivity graph \n",
    "neighbors = {}\n",
    "neighbors[1] = {2,3} \n",
    "neighbors[2] = {1,3,4} \n",
    "neighbors[3] = {1,2,4,5,6}\n",
    "neighbors[4] = {2,3,4}\n",
    "neighbors[5] = {3,4,6}\n",
    "print(neighbors)\n",
    "\n",
    "# Function for randomly selecting from possible choices \n",
    "import random\n",
    "print(random.choice((1,2)))\n",
    "print(random.choice((1,2,5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add an __init__ function that initializes a new type of environment called GraphPark and implements the move action as described above. The locations are still integers so you can place a Food thing at any particular location as you did before. Modify the show function to display all locations as well as where the Blind Dog \n",
    "and Food are. You should be able to re-use either the BlindDog or BlindDogNew implementation without changes with this new environment. Run the same simulation scenario as above in terms of placement of food and dog and alternating between running and showing as we did above. Note that the exact sequence of locations of the simulation will vary between different runs due to the non-determinism in the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:[]\n",
      "1:[<BlindNewDog>]\n",
      "2:[]\n",
      "3:[<Food>]\n",
      "4:[]\n",
      "BlindNewDog decided to move at location: 1\n",
      "0:[]\n",
      "1:[]\n",
      "2:[]\n",
      "3:[<BlindNewDog>, <Food>]\n",
      "4:[]\n",
      "BlindNewDog ate Food at location: 1\n",
      "0:[]\n",
      "1:[]\n",
      "2:[]\n",
      "3:[<BlindNewDog>]\n",
      "4:[]\n",
      "BlindNewDog decided to move at location: 3\n",
      "0:[]\n",
      "1:[]\n",
      "2:[]\n",
      "3:[]\n",
      "4:[<BlindNewDog>]\n",
      "BlindNewDog decided to move at location: 4\n",
      "0:[]\n",
      "1:[]\n",
      "2:[<BlindNewDog>]\n",
      "3:[]\n",
      "4:[]\n"
     ]
    }
   ],
   "source": [
    "# Q2 ANSWER GOES HERE (GraphPark and simulation code) \n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "class GraphPark(Environment):\n",
    "    location = 1\n",
    "\n",
    "    def __init__(self):\n",
    "        self.things = []\n",
    "        self.agents = []\n",
    "\n",
    "    def percept(self, agent):\n",
    "        '''return a list of things that are in our agent's location'''\n",
    "        things = self.list_things_at(agent.location)\n",
    "        return things\n",
    "\n",
    "    \n",
    "\n",
    "    def execute_action(self, agent, action):\n",
    "        '''changes the state of the environment based on what the agent does.'''\n",
    "\n",
    "        neighbors = {}\n",
    "        neighbors[0] = {1,2}\n",
    "        neighbors[1] = {0,2,3} \n",
    "        neighbors[2] = {0,1,4}\n",
    "        neighbors[3] = {1,4}\n",
    "        neighbors[4] = {2,3}\n",
    "\n",
    "        if action == \"move\": # new action \"move\" action is supported by the graphpark Environment \n",
    "            print('{} decided to {} at location: {}'.format(str(agent)[1:-1], action, agent.location))\n",
    "            #GraphPark.move(BlindNewDog)\n",
    "            #print(\"agent location before\", agent.location)\n",
    "            neighborstuple = tuple(neighbors[agent.location])\n",
    "          #  print(neighborstuple)\n",
    "            agent.location = random.choice(neighborstuple)\n",
    "           # print(\"agent location after\", agent.location)\n",
    "        elif action == \"eat\":\n",
    "            items = self.list_things_at(agent.location, tclass=Food)\n",
    "            if len(items) != 0:\n",
    "                if agent.eat(items[0]): #Have the dog eat the first item\n",
    "                    print('{} ate {} at location: {}'\n",
    "                          .format(str(agent)[1:-1], str(items[0])[1:-1], BlindNewDog.location))\n",
    "                    self.delete_thing(items[0]) #Delete it from the Park after.\n",
    "\n",
    "    def is_done(self):\n",
    "        # simulation without explicit ending condition\n",
    "        return False\n",
    "    \n",
    "    def show(self, max_location): \n",
    "        for i in range(0,max_location): \n",
    "            print('{}:{}'.format(str(i), str(self.list_things_at(i))))  #modify \n",
    "\n",
    "\n",
    "class BlindNewDog(Agent):\n",
    "    location = 1\n",
    "        \n",
    "    def eat(self, thing):\n",
    "        '''returns True upon success or False otherwise'''\n",
    "        if isinstance(thing, Food):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "class HungryProgramNew:\n",
    "    \n",
    "    def __init__(self): \n",
    "        self.hungry = True \n",
    "\n",
    "    def program(self,percepts):\n",
    "        '''Returns an action based on it's percepts'''\n",
    "        for p in percepts: # first eat  - you're a dog!\n",
    "            if isinstance(p, Food):\n",
    "                self.hungry = False \n",
    "                return 'eat'\n",
    "           # if (self.hungry == False):\n",
    "               # return 'move up'\n",
    "        return 'move'\n",
    "\n",
    "\n",
    "    def __call__(self, precepts): \n",
    "        return self.program(precepts)\n",
    "\n",
    "\n",
    "park = GraphPark()\n",
    "dog = BlindNewDog(HungryProgramNew())\n",
    "food = Food()\n",
    "park.add_thing(dog, 1)\n",
    "park.add_thing(food, 3)\n",
    "park.show(5)\n",
    "park.run(1)\n",
    "park.show(5) \n",
    "park.run(1)\n",
    "park.show(5)\n",
    "park.run(1) \n",
    "park.show(5)\n",
    "park.run(1)\n",
    "park.show(5)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Advanced) Ideas for further exploration of agents (no points) \n",
    "\n",
    "\n",
    "1. Add a graphical representation of the graph using the pyvis package and show the movement of the BlindDog \n",
    "2. Change the implementation of the GraphPark environment and the BlindDog so that the agent perceives the location it is in (perhaps there is a unique smell to each location) as well as the locations that are neighbors. As the agent randomly wonders around the graph it should build a model remembering the locations where there was a food item.\n",
    "3. Add a new type of item called Tree. When the BlindDog sense a tree it urinates and changes the Tree to a Stained Tree. \n",
    "4. Make a 2D park \n",
    "5. Add multiple dogs to the simulation and add random appearance of food in particular locations \n",
    "6. Write a goal-based agent version of the dog \n",
    "7. Write a utility-based agent version of the dog \n",
    "\n",
    "\n",
    "Do not hesitate to contact me if you are looking for additional ideas for further work. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import heapq\n",
    "from collections import defaultdict, deque, Counter\n",
    "\n",
    "\n",
    "class Problem(object):\n",
    "    \"\"\"The abstract class for a formal problem. A new domain subclasses this,\n",
    "    overriding `actions` and `results`, and perhaps other methods.\n",
    "    The default heuristic is 0 and the default action cost is 1 for all states.\n",
    "    When yiou create an instance of a subclass, specify `initial`, and `goal` states \n",
    "    (or give an `is_goal` method) and perhaps other keyword args for the subclass.\"\"\"\n",
    "\n",
    "    def __init__(self, initial=None, goal=None, **kwds): \n",
    "        self.__dict__.update(initial=initial, goal=goal, **kwds) \n",
    "        \n",
    "    def actions(self, state):        raise NotImplementedError\n",
    "    def result(self, state, action): raise NotImplementedError\n",
    "    def is_goal(self, state):        return state == self.goal\n",
    "    def action_cost(self, s, a, s1): return 1\n",
    "    def h(self, node):               return 0\n",
    "    \n",
    "    def __str__(self):\n",
    "        return '{}({!r}, {!r})'.format(\n",
    "            type(self).__name__, self.initial, self.goal)\n",
    "    \n",
    "\n",
    "class Node:\n",
    "    \"A Node in a search tree.\"\n",
    "    def __init__(self, state, parent=None, action=None, path_cost=0):\n",
    "        self.__dict__.update(state=state, parent=parent, action=action, path_cost=path_cost)\n",
    "\n",
    "    def __repr__(self): return '<{}>'.format(self.state)\n",
    "    def __len__(self): return 0 if self.parent is None else (1 + len(self.parent))\n",
    "    def __lt__(self, other): return self.path_cost < other.path_cost\n",
    "    \n",
    "    \n",
    "failure = Node('failure', path_cost=math.inf) # Indicates an algorithm couldn't find a solution.\n",
    "cutoff  = Node('cutoff',  path_cost=math.inf) # Indicates iterative deepening search was cut off.\n",
    "    \n",
    "    \n",
    "def expand(problem, node):\n",
    "    \"Expand a node, generating the children nodes.\"\n",
    "    s = node.state\n",
    "    for action in problem.actions(s):\n",
    "        s1 = problem.result(s, action)\n",
    "        cost = node.path_cost + problem.action_cost(s, action, s1)\n",
    "        yield Node(s1, node, action, cost)\n",
    "        \n",
    "\n",
    "def path_actions(node):\n",
    "    \"The sequence of actions to get to this node.\"\n",
    "    if node.parent is None:\n",
    "        return []  \n",
    "    return path_actions(node.parent) + [node.action]\n",
    "\n",
    "\n",
    "def path_states(node):\n",
    "    \"The sequence of states to get to this node.\"\n",
    "    if node in (cutoff, failure, None): \n",
    "        return []\n",
    "    return path_states(node.parent) + [node.state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIFOQueue = deque\n",
    "\n",
    "LIFOQueue = list\n",
    "\n",
    "class PriorityQueue:\n",
    "    \"\"\"A queue in which the item with minimum f(item) is always popped first.\"\"\"\n",
    "\n",
    "    def __init__(self, items=(), key=lambda x: x): \n",
    "        self.key = key\n",
    "        self.items = [] # a heap of (score, item) pairs\n",
    "        for item in items:\n",
    "            self.add(item)\n",
    "         \n",
    "    def add(self, item):\n",
    "        \"\"\"Add item to the queuez.\"\"\"\n",
    "        pair = (self.key(item), item)\n",
    "        heapq.heappush(self.items, pair)\n",
    "\n",
    "    def pop(self):\n",
    "        \"\"\"Pop and return the item with min f(item) value.\"\"\"\n",
    "        return heapq.heappop(self.items)[1]\n",
    "    \n",
    "    def top(self): return self.items[0][1]\n",
    "\n",
    "    def __len__(self): return len(self.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_first_search(problem, f):\n",
    "    \"Search nodes with minimum f(node) value first.\"\n",
    "    node = Node(problem.initial)\n",
    "    frontier = PriorityQueue([node], key=f)\n",
    "    print(frontier)\n",
    "    reached = {problem.initial: node}\n",
    "    while frontier:\n",
    "        node = frontier.pop()\n",
    "        if problem.is_goal(node.state):\n",
    "            return node\n",
    "        for child in expand(problem, node):\n",
    "            s = child.state\n",
    "            if s not in reached or child.path_cost < reached[s].path_cost:\n",
    "                reached[s] = child\n",
    "                frontier.add(child)\n",
    "    return failure\n",
    "\n",
    "\n",
    "def best_first_tree_search(problem, f):\n",
    "    \"A version of best_first_search without the `reached` table.\"\n",
    "    frontier = PriorityQueue([Node(problem.initial)], key=f)\n",
    "    while frontier:\n",
    "        node = frontier.pop()\n",
    "        if problem.is_goal(node.state):\n",
    "            return node\n",
    "        for child in expand(problem, node):\n",
    "            if not is_cycle(child):\n",
    "                frontier.add(child)\n",
    "    return failure\n",
    "\n",
    "\n",
    "def g(n): return n.path_cost\n",
    "\n",
    "\n",
    "def astar_search(problem, h=None):\n",
    "    \"\"\"Search nodes with minimum f(n) = g(n) + h(n).\"\"\"\n",
    "    h = h or problem.h\n",
    "    return best_first_search(problem, f=lambda n: g(n) + h(n))\n",
    "\n",
    "\n",
    "def astar_tree_search(problem, h=None):\n",
    "    \"\"\"Search nodes with minimum f(n) = g(n) + h(n), with no `reached` table.\"\"\"\n",
    "    h = h or problem.h\n",
    "    return best_first_tree_search(problem, f=lambda n: g(n) + h(n))\n",
    "\n",
    "\n",
    "def weighted_astar_search(problem, h=None, weight=1.4):\n",
    "    \"\"\"Search nodes with minimum f(n) = g(n) + weight * h(n).\"\"\"\n",
    "    h = h or problem.h\n",
    "    return best_first_search(problem, f=lambda n: g(n) + weight * h(n))\n",
    "\n",
    "        \n",
    "def greedy_bfs(problem, h=None):\n",
    "    \"\"\"Search nodes with minimum h(n).\"\"\"\n",
    "    h = h or problem.h\n",
    "    return best_first_search(problem, f=h)\n",
    "\n",
    "\n",
    "def uniform_cost_search(problem):\n",
    "    \"Search nodes with minimum path cost first.\"\n",
    "    return best_first_search(problem, f=g)\n",
    "\n",
    "\n",
    "def breadth_first_bfs(problem):\n",
    "    \"Search shallowest nodes in the search tree first; using best-first.\"\n",
    "    return best_first_search(problem, f=len)\n",
    "\n",
    "\n",
    "def depth_first_bfs(problem):\n",
    "    \"Search deepest nodes in the search tree first; using best-first.\"\n",
    "    return best_first_search(problem, f=lambda n: -len(n))\n",
    "\n",
    "\n",
    "def is_cycle(node, k=30):\n",
    "    \"Does this node form a cycle of length k or less?\"\n",
    "    def find_cycle(ancestor, k):\n",
    "        return (ancestor is not None and k > 0 and\n",
    "                (ancestor.state == node.state or find_cycle(ancestor.parent, k - 1)))\n",
    "    return find_cycle(node.parent, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RouteProblem(Problem):\n",
    "    \"\"\"A problem to find a route between locations on a `Map`.\n",
    "    Create a problem with RouteProblem(start, goal, map=Map(...)}).\n",
    "    States are the vertexes in the Map graph; actions are destination states.\"\"\"\n",
    "    \n",
    "    def actions(self, state): \n",
    "        \"\"\"The places neighboring `state`.\"\"\"\n",
    "        return self.map.neighbors[state]\n",
    "    \n",
    "    def result(self, state, action):\n",
    "        \"\"\"Go to the `action` place, if the map says that is possible.\"\"\"\n",
    "        return action if action in self.map.neighbors[state] else state\n",
    "    \n",
    "    def action_cost(self, s, action, s1):\n",
    "        \"\"\"The distance (cost) to go from s to s1.\"\"\"\n",
    "        return self.map.distances[s, s1]\n",
    "    \n",
    "    def h(self, node):\n",
    "        \"Straight-line distance between state and the goal.\"\n",
    "        locs = self.map.locations\n",
    "        return straight_line_distance(locs[node.state], locs[self.goal])\n",
    "    \n",
    "    \n",
    "def straight_line_distance(A, B):\n",
    "    \"Straight-line distance between two points.\"\n",
    "    return sum(abs(a - b)**2 for (a, b) in zip(A, B)) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Map:\n",
    "    \"\"\"A map of places in a 2D world: a graph with vertexes and links between them. \n",
    "    In `Map(links, locations)`, `links` can be either [(v1, v2)...] pairs, \n",
    "    or a {(v1, v2): distance...} dict. Optional `locations` can be {v1: (x, y)} \n",
    "    If `directed=False` then for every (v1, v2) link, we add a (v2, v1) link.\"\"\"\n",
    "\n",
    "    def __init__(self, links, locations=None, directed=False):\n",
    "        if not hasattr(links, 'items'): # Distances are 1 by default\n",
    "            links = {link: 1 for link in links}\n",
    "        if not directed:\n",
    "            for (v1, v2) in list(links):\n",
    "                links[v2, v1] = links[v1, v2]\n",
    "        self.distances = links\n",
    "        self.neighbors = multimap(links)\n",
    "        self.locations = locations or defaultdict(lambda: (0, 0))\n",
    "\n",
    "        \n",
    "def multimap(pairs) -> dict:\n",
    "    \"Given (key, val) pairs, make a dict of {key: [val,...]}.\"\n",
    "    result = defaultdict(list)\n",
    "    for key, val in pairs:\n",
    "        result[key].append(val)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some specific RouteProblems\n",
    "\n",
    "romania = Map(\n",
    "    {('O', 'Z'):  71, ('O', 'S'): 151, ('A', 'Z'): 75, ('A', 'S'): 140, ('A', 'T'): 118, \n",
    "     ('L', 'T'): 111, ('L', 'M'):  70, ('D', 'M'): 75, ('C', 'D'): 120, ('C', 'R'): 146, \n",
    "     ('C', 'P'): 138, ('R', 'S'):  80, ('F', 'S'): 99, ('B', 'F'): 211, ('B', 'P'): 101, \n",
    "     ('B', 'G'):  90, ('B', 'U'):  85, ('H', 'U'): 98, ('E', 'H'):  86, ('U', 'V'): 142, \n",
    "     ('I', 'V'):  92, ('I', 'N'):  87, ('P', 'R'): 97},\n",
    "    {'A': ( 76, 497), 'B': (400, 327), 'C': (246, 285), 'D': (160, 296), 'E': (558, 294), \n",
    "     'F': (285, 460), 'G': (368, 257), 'H': (548, 355), 'I': (488, 535), 'L': (162, 379),\n",
    "     'M': (160, 343), 'N': (407, 561), 'O': (117, 580), 'P': (311, 372), 'R': (227, 412),\n",
    "     'S': (187, 463), 'T': ( 83, 414), 'U': (471, 363), 'V': (535, 473), 'Z': (92, 539)})\n",
    "\n",
    "\n",
    "r0 = RouteProblem('A', 'A', map=romania)\n",
    "r1 = RouteProblem('A', 'B', map=romania)\n",
    "r2 = RouteProblem('N', 'L', map=romania)\n",
    "r3 = RouteProblem('E', 'T', map=romania)\n",
    "r4 = RouteProblem('O', 'M', map=romania)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.PriorityQueue object at 0x1032837b8>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['A', 'S', 'R', 'P', 'B']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_states(uniform_cost_search(r1)) # Lowest-cost path from Arab to Bucharest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.PriorityQueue object at 0x1032a5c18>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['A', 'S', 'F', 'B']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_states(breadth_first_bfs(r1)) # BFS path from Arab to Bucharest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.PriorityQueue object at 0x1032a5d30>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['N', 'I', 'V', 'U', 'B', 'P', 'C', 'D', 'M', 'L']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_states(breadth_first_bfs(r2)) # Breadth-first "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 3 Search (Basic) 1.0 point \n",
    "\n",
    "Modify the map of Romania above by adding a connection from Sibiu to Bucharest with a cost of 280. Run **BFS** and **DFS** search for the r1 pair (Arad to Bucharest) for the new map (RomaniaNew) and return the solution path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3 ANSWER GOES HERE (RomaniaNew, best_first_search with frontier printing, ) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 4 Search (Basic) 1.0 point \n",
    "\n",
    "Modify the code of best_first_search to print the states that are in the frontier at every iteration of the search algorithm as a list. Repeat the two searches from the previous question showing how the frontier evolves over time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4 ANSWER GOES HERE (best_first_search with printing of frontier)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 5 Search (Expected) 1.0 point \n",
    "\n",
    "Write your own search algorithm called **random_search**. In this search algorithm the node selected for expansion is selected randomly from the frontier (with uniform probability among the possible nodes). \n",
    "\n",
    "Use the new algorithm to run it for the r1 pair (Arad to Buchararest) on the RomaniaNew map also showing \n",
    "how the frontier evolves over time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5 ANSWER GOES HERE (random_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 6 Search (Expected) 1.0 point \n",
    "\n",
    "Let's now look into generating random instances of the RouteMap problem. \n",
    "Implement the following algorithm: \n",
    "\n",
    "1. Each \"city\" is labeled with a letter of the aphabet for a total of 26 cities.\n",
    "2. The initial city is A and the goal city is Z \n",
    "3. Each city is randomly placed on a grid by drawing from two uniform random distributions between 0 and 100 \n",
    "for the x and y dimensions.  \n",
    "4. To form connections for each city C select the 5 closest cities (based on the Euclidean distance). Select 3 out of the 5 closest cities randomly and add connections between them and the the city C if there is not \n",
    "already a connection. Repeated this process for all 26 cities starting from A and ending with Z. \n",
    "5. Assign the cost of each connection to be the euclidean distance between the two connected cities \n",
    "6. The euclidean distance to the goal city Z will be the heuristic as is common with Route Mapping problems \n",
    "\n",
    "\n",
    "Using this procedure generate 100 random maps and run the following search algorithms (BFS, DFS, and A* search). \n",
    "For each algorithm report: the number of maps for which it found a solution (sometimes the connectivity \n",
    "heuristic generates a map for which no route from A to Z can be found), and the time complexity i.e the number of nodes generated during the search. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6 ANSWER GOES HERE (random_map_generator, report statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 7 Search (Advanced) 1.0 point \n",
    "\n",
    "Create a visualization for RouteMap problems that displays the cities as circles with associated letters and connects them with lines. Use either the matplotlib or bokeh Python frameworks for creating plots. Use color to display the frontier (red) and solution path (blue). \n",
    "\n",
    "Use your visualization to show different stages of the algorithms for both a randomly generated map as well as the map of Romania. Write accompanying text that describe how the visualizations show different aspects of the search algorithms (only consider BFS, DFS, and A*-search) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7 ANSWER GOES HERE (map visualization and explanation of how search algorithms work using it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 8 CSP (Basic) 1.0 point \n",
    "\n",
    "\n",
    "Let's look at a simple basic implemetation of recursive backgtracking search for solving CSP problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isComplete(assignment):\n",
    "    return None not in (assignment.values())\n",
    "\n",
    "def select_unassigned_variable(variables, assignment):\n",
    "    for var in variables:\n",
    "        if assignment[var] is None:\n",
    "            return var\n",
    "\n",
    "def is_consistent(assignment, constraints):\n",
    "    for constraint_violated in constraints:\n",
    "        if constraint_violated(assignment):\n",
    "          return False\n",
    "    return True\n",
    "\n",
    "def init_assignment(csp):\n",
    "    assignment = {}\n",
    "    for var in csp[\"VARIABLES\"]:\n",
    "        assignment[var] = None\n",
    "    return assignment\n",
    "\n",
    "def add_constraint(csp, constraint): \n",
    "    csp['CONSTRAINTS'].append(constraint)\n",
    "    \n",
    "def recursive_backtracking(assignment, csp):\n",
    "    if isComplete(assignment):\n",
    "        return assignment\n",
    "    var = select_unassigned_variable(csp[\"VARIABLES\"], assignment)\n",
    "    for value in csp[\"DOMAINS\"]:\n",
    "        assignment[var] = value\n",
    "        if is_consistent(assignment, csp[\"CONSTRAINTS\"]):\n",
    "            result = recursive_backtracking(assignment, csp)\n",
    "            if result != \"FAILURE\":\n",
    "                return result\n",
    "        assignment[var] = None\n",
    "    return \"FAILURE\"\n",
    "\n",
    "def binary_constraint(var_pair, violations):\n",
    "    (v1,v2) = var_pair\n",
    "    return lambda asmt: (asmt[v1], asmt[v2]) in violations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this implementation to solve the Australia map coloring problem. Before working on this question make sure you understand how the code works. \n",
    "\n",
    "1. Print the assignment of variables to values during the recursive backtracking search \n",
    "2. Suppose that we want to enforce that Westeran Australia (WA) should be blue in our solution. \n",
    "Create an initial assignment to pass as the first argument to recursive backtracking to achieve that. \n",
    "3. Add a unary constraint function. Similarly to binary constraint it should return a function that takes \n",
    "as input an assignment and return true if the assignment violates the constraint. Show how this new unary \n",
    "constraint can be used to enforce that WA is blue and T is blue in the resulting solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result {'WA': 'red', 'NT': 'green', 'Q': 'red', 'NSW': 'green', 'V': 'red', 'SA': 'blue', 'T': 'green'}\n"
     ]
    }
   ],
   "source": [
    "csp1 = {\"VARIABLES\": [\"WA\", \"NT\", \"Q\", \"NSW\", \"V\", \"SA\", \"T\"],\n",
    "        \"DOMAINS\": [\"red\", \"green\", \"blue\"],\n",
    "        \"CONSTRAINTS\": []}\n",
    "\n",
    "violations = {('red','red'), ('green','green'), ('blue','blue')}\n",
    "\n",
    "for (v1,v2) in [('WA', 'NT'), ('WA', 'SA'), \n",
    "                ('NT', 'SA'), ('NT', 'Q'), \n",
    "                ('SA', 'Q'), ('SA', 'NSW'), \n",
    "                ('SA', 'V'),('Q', 'NSW'), \n",
    "                ('V', 'T')]: \n",
    "    add_constraint(csp1, binary_constraint((v1,v2), violations))\n",
    "\n",
    "result = recursive_backtracking(init_assignment(csp1), csp1)\n",
    "print('Result', result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8 ANSWER GOES HERE (modifications to CSP code and definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 9 CSP (Expected) 1.0 point \n",
    "\n",
    "The send more money puzzle is based on the following cryptarithmetic equation: \n",
    "\n",
    "&nbsp; SEND<br>\n",
    " +MORE<br>\n",
    " MONEY<br>\n",
    " \n",
    "Each letter is variable with domain the digits 0-9. Each letter must be assigned a different digit \n",
    "in such a way that final assignment satisfies the equation. \n",
    "\n",
    "\n",
    "For example here is a solution\n",
    " {'S': 9, 'E': 5, 'N': 6, 'D': 7, 'M': 1, 'O': 0, 'R': 8, 'Y': 2}\n",
    "\n",
    "9567+1085 = 10652\n",
    "\n",
    "\n",
    "Using the code above express and solve this puzzle. You can define your own constraint function specific \n",
    "to this type of problem. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9 ANSWER GOES HERE (modifications to CSP code and definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 10 CSP (Advanced) 1.0 point "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Express solving CSP problems as search problems and use the search algorithm code that is provided \n",
    "in this notebook to solve the Australia map problem. The specification of the CSP should use the same \n",
    "convention as above but you can transform the provided information as needed to use the search algorithm. \n",
    "Show how **BFS** and **DFS** can be used to solve the Australia map coloring problem. Important note: your \n",
    "approach should be general and allow the solution of any CSP problem with variables, domains, and constraints \n",
    "specified as above. It should **NOT** only solve the Australia map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
